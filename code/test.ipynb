{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries and helper functions\n",
    "import pandas as pd\n",
    "import os\n",
    "from helper_functions import make_can_df, add_time_diff_per_aid_col, add_actual_attack_col, add_kde_val_col, add_gauss_val_col, get_results_binning, unpickle\n",
    "import json\n",
    "import tqdm\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading and saving normal traffic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(directory, exclude=[], file_condition=lambda file_name: True):\n",
    "    print(\"Loading data from directory: \" + directory)\n",
    "    df_aggregation = []\n",
    "\n",
    "    for file_name in os.listdir(directory):\n",
    "        if file_condition(file_name) and not any(excl in file_name for excl in exclude):\n",
    "            print(\"Loading file: \" + file_name)\n",
    "            df = make_can_df(os.path.join(directory, file_name))\n",
    "            df = add_time_diff_per_aid_col(df)\n",
    "            df_aggregation.append(df)\n",
    "            \n",
    "    return df_aggregation\n",
    "\n",
    "\n",
    "\n",
    "def load_and_save_training_data(directory):\n",
    "    if os.path.exists(directory + 'training_data.csv'):\n",
    "        print(\"Training data already exists. Loading training data from: \" + directory + 'training_data.csv')\n",
    "        return pd.read_csv(directory + 'training_data.csv')\n",
    "        \n",
    "    df_aggregation = load_data(directory, file_condition=lambda file_name: \"dyno\" in file_name)\n",
    "    # Concatenate all training datasets on the dyno\n",
    "    df_training = pd.concat(df_aggregation)\n",
    "    training_data = df_training[[\"time\", \"aid\", \"time_diffs\"]]\n",
    "    save_file = 'training_data.csv'\n",
    "    print(\"Saving training data to: \" + save_file)\n",
    "    training_data.to_csv(directory + save_file)\n",
    "    return training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = load_and_save_training_data('/home/jbc0071/Documents/can-time-based-ids-benchmark/data/ambient/')\n",
    "display(training_data)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \"Train\" model: calculating stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.covariance import EllipticEnvelope\n",
    "import scipy.stats\n",
    "\n",
    "def preprocess(df, aid):\n",
    "    \"\"\"\n",
    "    Preprocesses the data by removing outliers.\n",
    "    \"\"\"\n",
    "    time_diffs = df[df.aid==aid].time_diffs.values\n",
    "    print(\"before: \", len(time_diffs))\n",
    "\n",
    "    # identify outliers in the dataset\n",
    "    ee = EllipticEnvelope(contamination=0.0001, support_fraction=0.999) # support_fraction=0.99\n",
    "    inliers = ee.fit_predict(time_diffs.reshape(-1, 1))\n",
    "\n",
    "    # select all rows that are not outliers\n",
    "    mask = inliers != -1\n",
    "    outliers = sum(mask == False)\n",
    "    print(\"outliers: \", outliers, 100*outliers/len(time_diffs))\n",
    "\n",
    "    time_diffs = time_diffs[mask]\n",
    "    # summarize the shape of the updated training dataset\n",
    "    print(\"after: \", len(time_diffs))\n",
    "\n",
    "    return time_diffs\n",
    "\n",
    "\n",
    "def calculate_statistics(time_diffs):\n",
    "    \"\"\"\n",
    "    Returns a dictionary including the mean of its time_diffs, standard deviation of its time_diffs\n",
    "    and KDE of its time_diffs\n",
    "    \"\"\"\n",
    "    aid_dict = {'mu': time_diffs.mean(), 'std': time_diffs.std(), 'kde': scipy.stats.gaussian_kde(time_diffs), 'gauss': scipy.stats.norm(loc = time_diffs.mean(), scale = time_diffs.std())}\n",
    "    aid_dict[\"y_thresholds_kde\"] = {}\n",
    "    aid_dict[\"y_thresholds_gauss\"] = {}\n",
    "    return aid_dict\n",
    "\n",
    "\n",
    "def calculate_statistics_for_each_aid(data):\n",
    "    # Get a list of unique aids in the data\n",
    "    unique_aids = data['aid'].unique()\n",
    "    # Preprocess the data and calculate statistics for each unique aid\n",
    "    stats = {aid: calculate_statistics(preprocess(data, aid)) for aid in unique_aids}\n",
    "    # data = [add_kde_val_col(data[i], stats) for i in range(len(data))]\n",
    "    # data = [add_gauss_val_col(data[i], stats) for i in range(len(data))]\n",
    "    \n",
    "    return stats\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_stats = calculate_statistics_for_each_aid(training_data)\n",
    "display(training_data_stats)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and Annotating Attack Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotate_attack_data(attack_data, injection_intervals):\n",
    "    \"\"\"\n",
    "    Annotates the attack data based on the injection intervals.\n",
    "    \"\"\"\n",
    "    for index, row in injection_intervals.iterrows():\n",
    "        aid = row['aid']\n",
    "        payload = row['payload']\n",
    "        intervals = [(row['start_time'], row['end_time'])]\n",
    "        attack_data = add_actual_attack_col(attack_data, intervals, aid, payload)\n",
    "    return attack_data\n",
    "\n",
    "def load_and_annotate_attack_data(directory, metadata_file):\n",
    "    # Load the attack data\n",
    "    df_aggregation = load_data(directory, exclude=['masquerade', 'accelerator', 'metadata', metadata_file])\n",
    "\n",
    "    # Load the injection intervals from the metadata file\n",
    "    with open(os.path.join(directory, metadata_file), \"r\") as read_file:\n",
    "        attack_dict = json.load(read_file)\n",
    "\n",
    "    attack_metadata = []\n",
    "    count = 0  # Initialize count here\n",
    "    for file_name in os.listdir(directory):\n",
    "        file_base = file_name[:-4]\n",
    "        if file_base not in attack_dict:\n",
    "            continue\n",
    "        if \"masquerade\" in file_name or \"accelerator\" in file_name:\n",
    "            continue\n",
    "\n",
    "        metadata = attack_dict[file_base]\n",
    "        if metadata[\"injection_id\"] != \"XXX\":\n",
    "            injection_id = int(metadata[\"injection_id\"], 16)\n",
    "        else:\n",
    "            injection_id = \"XXX\"\n",
    "\n",
    "        # From metadata file\n",
    "        attack_metadata.append([tuple(metadata[\"injection_interval\"])])\n",
    "\n",
    "        # Add column to each attack dataframe to indicate attack (True) or non-attack (False) for each signal\n",
    "        df_aggregation[count] = add_actual_attack_col(df_aggregation[count], attack_metadata[count], injection_id, metadata[\"injection_data_str\"])\n",
    "\n",
    "        count += 1  # Increment count here, inside the loop where you add items to your lists\n",
    "\n",
    "    return df_aggregation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attack_data = load_and_annotate_attack_data('../data/attacks', 'capture_metadata.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Sample attack data from 'reverse_light_on_attack_1.log':\")\n",
    "display(attack_data[2].head())\n",
    "\n",
    "print(\"\\nHow many have an actual attack:\")\n",
    "print(len(attack_data[2][attack_data[2].actual_attack == True]))\n",
    "\n",
    "print(\"\\nHow many do not have an actual attack:\")\n",
    "print(len(attack_data[2][attack_data[2].actual_attack == False]))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supporting functions for detecting attacks on data already collected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def get_results_binning(attack_list, D, n=6):\n",
    "    \"\"\"\n",
    "    Simplified binning detection method that returns the results directly\n",
    "    \"\"\"\n",
    "    results_binning = {}\n",
    "\n",
    "    for i, attack in enumerate(attack_list):\n",
    "        confusion_matrix_ = alert_by_bin(attack, D, n)\n",
    "        precision = confusion_matrix_[1,1] / (confusion_matrix_[1,1] + confusion_matrix_[0,1])\n",
    "        recall = confusion_matrix_[1,1] / (confusion_matrix_[1,1] + confusion_matrix_[1,0])\n",
    "        false_positive = confusion_matrix_[0,1] / (confusion_matrix_[0,1] + confusion_matrix_[0,0])\n",
    "\n",
    "        results_binning[i+1] = {\n",
    "            'cm': confusion_matrix_,\n",
    "            'prec': precision,\n",
    "            'recall': recall,\n",
    "            'false_pos': false_positive\n",
    "        }\n",
    "\n",
    "    return results_binning\n",
    "\n",
    "def alert_by_bin(df, D, n=6):\n",
    "    \"\"\"\n",
    "    Checks for time windows of length mu*4 (where mu is average time_diff for aid) with 6 or more signals\n",
    "    \"\"\"\n",
    "    cm = np.array([[0,0], [0,0]])\n",
    "    \n",
    "    for aid in df.aid.unique():\n",
    "        df_test = df[df.aid == aid]\n",
    "        df_test['predicted_attack'] = df_test.time_diffs.rolling(n).sum() <= D[aid]['mu']*4\n",
    "\n",
    "        cm += confusion_matrix(df_test['actual_attack'], df_test['predicted_attack'], labels = [0,1])\n",
    "    \n",
    "    return cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "# def detect_anomalies(models, attack_data, detection_method):\n",
    "#     if detection_method == 'Binning':\n",
    "#         return detect_anomalies_binning(models, attack_data)\n",
    "#     # elif detection_method == 'Gaussian':\n",
    "#     #     return detect_anomalies_gaussian(models, attack_data)\n",
    "#     else:\n",
    "#         raise ValueError(f\"Unknown detection method: {detection_method}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Live Binning Attack Detection\n",
    "This is implemented through a binning strategy for anomaly detection, where each bin corresponds \\\n",
    "to a time window of length mu*4. If a bin contains n or more messages, it's considered anomalous \\\n",
    "and marked as a potential attack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinningAttackDetector:\n",
    "    \"\"\"\n",
    "    This class implements a binning strategy for anomaly detection, where each bin corresponds \n",
    "    to a time window of length mu*4. If a bin contains n or more messages, it's considered \n",
    "    anomalous and marked as a potential attack.\n",
    "    \"\"\"\n",
    "    def __init__(self, d, n=6):\n",
    "        self.d = d\n",
    "        self.n = n\n",
    "        self.frames = {}\n",
    "\n",
    "    def process_frame(self, frame):\n",
    "        aid = frame['aid']\n",
    "        time_diff = frame['time_diffs']\n",
    "\n",
    "        # Initialize memory for this aid if it doesn't exist\n",
    "        if aid not in self.frames:\n",
    "            self.frames[aid] = []\n",
    "\n",
    "        # Add this frame to memory\n",
    "        self.frames[aid].append(time_diff)\n",
    "\n",
    "        # If we don't have enough frames yet, return 'not enough frames'\n",
    "        if len(self.frames[aid]) < self.n:\n",
    "            return 'not enough frames'\n",
    "\n",
    "        # If we have more than n frames, discard the oldest one\n",
    "        if len(self.frames[aid]) > self.n:\n",
    "            self.frames[aid].pop(0)\n",
    "\n",
    "        # Check if the sum of time_diffs for the last n frames is less than or equal to mu*4\n",
    "        if sum(self.frames[aid]) <= self.d[aid]['mu']*4:\n",
    "            return 'attack'\n",
    "        else:\n",
    "            return 'not attack'\n",
    "\n",
    "detector = BinningAttackDetector(training_data_stats)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def calculate_cm(df, predictions):\n",
    "    cm = confusion_matrix(df['actual_attack'], predictions, labels=[False, True])\n",
    "    return cm\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculation of metrics and total accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modified from get_results_binning in helper_functions.py\n",
    "def get_results_binning_updated(attack_list, D, n=6):\n",
    "    # Initialize results dictionary\n",
    "    results_binning = {}\n",
    "    for i in range(len(attack_list)):\n",
    "        results_binning[i+1] = {'cm': [0], 'recall': 0, 'prec': 0, 'false_pos': 0}\n",
    "    results_binning['total'] = {'cm': [0], 'recall': 0, 'prec': 0, 'false_pos': 0}\n",
    "\n",
    "    for i in range(len(attack_list)):\n",
    "        # Process each frame in the attack\n",
    "        predictions = []\n",
    "        for _, frame in attack_list[i].iterrows():\n",
    "            result = detector.process_frame(frame.to_dict())\n",
    "            predictions.append(result == 'attack')\n",
    "\n",
    "        # Calculate the confusion matrix\n",
    "        results_binning[i+1]['cm'] = calculate_cm(attack_list[i], predictions)\n",
    "\n",
    "        # Calculate the other metrics here...\n",
    "        denominator = (results_binning[i+1]['cm'][1, 1]+results_binning[i+1]['cm'][0, 1])\n",
    "        if denominator != 0:\n",
    "            results_binning[i+1]['prec'] = results_binning[i+1]['cm'][1, 1] / denominator\n",
    "        else:\n",
    "            results_binning[i+1]['prec'] = 0  # or np.nan, or however you want to handle this case\n",
    "\n",
    "        results_binning[i+1]['recall'] = results_binning[i+1]['cm'][1, 1] / \\\n",
    "            (results_binning[i+1]['cm'][1, 1]+results_binning[i+1]['cm'][1, 0])\n",
    "        results_binning[i+1]['false_pos'] = results_binning[i+1]['cm'][0, 1] / \\\n",
    "            (results_binning[i+1]['cm'][0, 1]+results_binning[i+1]['cm'][0, 0])\n",
    "        results_binning['total']['cm'] += results_binning[i+1]['cm']\n",
    "\n",
    "    # Calculate the total metrics here...\n",
    "    results_binning['total']['prec'] = results_binning['total']['cm'][1, 1] / \\\n",
    "        (results_binning['total']['cm'][1, 1] +\n",
    "         results_binning['total']['cm'][0, 1])\n",
    "    results_binning['total']['recall'] = results_binning['total']['cm'][1, 1] / \\\n",
    "        (results_binning['total']['cm'][1, 1] +\n",
    "         results_binning['total']['cm'][1, 0])\n",
    "    results_binning['total']['f1'] = 2*((results_binning['total']['prec']*results_binning['total']['recall'])/(\n",
    "        results_binning['total']['prec']+results_binning['total']['recall']))\n",
    "    results_binning['total']['false_pos'] = results_binning['total']['cm'][0, 1] / \\\n",
    "        (results_binning['total']['cm'][0, 1] +\n",
    "         results_binning['total']['cm'][0, 0])\n",
    "\n",
    "\n",
    "    return results_binning\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_binning = get_results_binning(attack_data, training_data_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_binning_new = get_results_binning_updated(attack_data, training_data_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(result_binning)\n",
    "display(result_binning_new['total'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unfinished below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# bin_results = []\n",
    "# for i in range(len(attack_data)):\n",
    "#     result_binning_new = get_results_binning_updated([attack_data[i]], training_data_stats)\n",
    "#     bin_results.append(result_binning_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for result in bin_results:\n",
    "#     display(result['total'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(attack_data)\n",
    "display(training_data_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_binning_final = unpickle(os.path.dirname(os.getcwd()) + \"/results_binning_final.pkl\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(results_binning_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = detect_and_save_results('results', training_data_stats, attack_data, 'Binning') # Detect anomalies and save the results\n",
    "# visualize_results(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
